<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F26%2FAIOHTTP-And-Tornado-Benchmark%2F</url>
    <content type="text"><![CDATA[测试环境 aiohttp==3.3.2 tornado==5.1 uvloop==0.11 硬件: Intel Xeon® CPU E5 v4, 3.6 gHz Python 3.6.5 [GCC 6.3.0] linux v4 测试方式 接收请求 测试持续时间: 300秒 AIOHTTP和Tornado都是Python中有名的 Python web框架和异步网络库。 这里做了一个简单的基准测试，测试代码逻辑比较简单：请求到达时进行睡眠（sleep）。 基准测试图片如下： 可以看到AIOHTTP + uvloop的组合比 tornado快了不少。 性能： AIOHTTP + uvloop &gt; AIOHTTP &gt; tornado + uvloop &gt; tornado newstyle &gt; tornado oldstyle 总结：AIOHTTP作为新一代异步框架，速度的确快了不少。当然，对于笔者来说， 性能是一方面，生态周边也是另一方面。 如果是入门阶段，在这两个框架选择，我建议使用AIOHTTP开发，。如果是熟练阶段，那么一切就都取决于你！ 参考]]></content>
  </entry>
  <entry>
    <title><![CDATA[Scrapy框架及反爬分享]]></title>
    <url>%2F2019%2F01%2F30%2FScrapy%2FScrapy%E6%A1%86%E6%9E%B6%E5%8F%8A%E5%8F%8D%E7%88%AC%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[1. 也许是知名度最高的框架：Scrapy 1.1 架构（Arichitecture） 从最简单的爬虫开始。 通常我们写一个爬虫的过程就是： 写个起始URL，然后用多线程发Requests请求，解析Response，产生新的请求就继续，产生需要的数据了就存下来。 那么Scrapy定了一套规范，把代码规范化，组件化，大大提高了代码的复用程度。 我们写爬虫脚本肯定会关注： 爬取策略：广度优先、深度优先、规则优先。 解析策略：基于DOM的规则式抽取(Xpath,Selector)、基于正则的抽取（Regex）、基于算法的抽取(readability, page-segmentation)。 去重策略：请求相似去重，结果相似去重;去重算法（hash，bloomfilter…）。 更新策略：监控网页是否更新，定时重爬。 存储：Mysql, HBase, MongoDB, ElasticSearch, Kafka, OSS… 监控： 网页是否正常爬取？监控关键指标：请求量，结果量，状态码，异常码…。 通知：爬虫运行状态通知（开始、结束、异常退出）。 调试： xpath写得对不对，结果对不对？用断点调试？最好能够可视化。 性能： 单线程、多线程、多进程、分布式。 反爬： UA反爬，IP反爬，验证码反爬等等。 写一个爬虫脚本没什么感觉，写十个、一百个甚至一千个并且还要维护这些代码的时候，肯定就受不了了。 大部分模块都是可以通用的。理想状态就是，我们只需要在写爬虫具体逻辑的时候，指定我们要用的功能模块，然后直接用现成的模块就可以了。这将大大减少写一个爬虫的时间和心智负担。 我这里基于我上面那个图又加了点东西： 按照图里的流程，我们就可以写出一个爬虫框架了。 Scrapy早就发现了这个问题，并且写了个框架出来。 这个图不那么直观，改成我的画法： ![image-20190130204424916](/Users/nikan/Library/Application Support/typora-user-images/image-20190130204424916.png) 这就是Scrapy最开始的架构。 1.1.2 Scrapy运行细节 参考：https://doc.scrapy.org/en/latest/intro/overview.html 写一个爬虫脚本quotes_spider.py： 12345678910111213141516171819import scrapyclass QuotesSpider(scrapy.Spider): name = "quotes" start_urls = [ 'http://quotes.toscrape.com/tag/humor/', ] def parse(self, response): for quote in response.css('div.quote'): yield &#123; 'text': quote.css('span.text::text').extract_first(), 'author': quote.xpath('span/small/text()').extract_first(), &#125; next_page = response.css('li.next a::attr("href")').extract_first() if next_page is not None: yield response.follow(next_page, self.parse) 然后在命令行文件目录下执行 1scrapy runspider quotes_spider.py -o quotes.json 就可以了。 这个是怎么做的？看一下scrapy的源代码： 123456789101112➜ Downloads cat $(which scrapy)#!/usr/local/opt/python3/bin/python3.6# -*- coding: utf-8 -*-import reimport sysfrom scrapy.cmdline import execute # 看下execute做了什么事情if __name__ == '__main__': sys.argv[0] = re.sub(r'(-script\.pyw?|\.exe)?$', '', sys.argv[0]) sys.exit(execute()) 1234567891011121314151617# 简化后的代码def execute(argv=None, settings=None): cmds = _get_commands_dict(settings, inproject) cmdname = _pop_command_name(argv) parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), \ conflict_handler='resolve') cmd = cmds[cmdname] settings.setdict(cmd.default_settings, priority='command') # 设置默认参数 cmd.settings = settings cmd.add_options(parser) opts, args = parser.parse_args(args=argv[1:]) # 解析参数 _run_print_help(parser, cmd.process_options, args, opts) # 运行cmd.process_options函数 cmd.crawler_process = CrawlerProcess(settings) _run_print_help(parser, _run_command, cmd, args, opts)# 运行_run_command函数 sys.exit(cmd.exitcode) 那么最后肯定是运行runspider这个命令了。 我们看到runspider.py里面的做法： 前面都是解析参数的过程，核心部分是crawler_process。 包括scrapy crawl也是这样的。 相信大多数人刚开始接触scrapy的时候以命令行方式调试都挺难受的，不利于调试。 我们可以尝试直接在代码里面跑脚本吗？可以。 参考：https://doc.scrapy.org/en/latest/topics/practices.html#run-scrapy-from-a-script 123456789101112import scrapyfrom scrapy.crawler import CrawlerProcessclass MySpider(scrapy.Spider): # Your spider definition ...# 获取项目配置process = CrawlerProcess(get_project_settings())# 执行process.crawl(MySpider)# 开始process.start() # the script will block here until the crawling is finished 这样利用Pycharm的断点调试功能就很方便了。 接下来我们从源代码的角度来理解Scrapy的架构 ![image-20190130204548687](/Users/nikan/Library/Application Support/typora-user-images/image-20190130204548687.png) 图中的名词都是Scrapy内部的组件名称。 1.2 爬虫（Spider） spider就是写爬取具体逻辑的，我们继承Spider这个类 定义一下开头的 start_requests跟 parse函数。 当然还有CrawlSpider，可以写规则来抽页面，写通用爬虫挺好用的。 1.3 引擎（Engine） Engine是Scrapy的核心 12345678910111213141516class ExecutionEngine(object): def __init__(self, crawler, spider_closed_callback): self.crawler = crawler self.settings = crawler.settings self.signals = crawler.signals self.logformatter = crawler.logformatter self.slot = None self.spider = None self.running = False self.paused = False self.scheduler_cls = load_object(self.settings['SCHEDULER']) # 队列 downloader_cls = load_object(self.settings['DOWNLOADER']) self.downloader = downloader_cls(crawler) # 下载器 self.scraper = Scraper(crawler) # 解析器 self._spider_closed_callback = spider_closed_callback 从爬虫打开后说起 1234567891011121314151617@defer.inlineCallbacksdef open_spider(self, spider, start_requests=(), close_if_idle=True): assert self.has_capacity(), "No free spider slot when opening %r" % \ spider.name logger.info("Spider opened", extra=&#123;'spider': spider&#125;) nextcall = CallLaterOnce(self._next_request, spider) scheduler = self.scheduler_cls.from_crawler(self.crawler) start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider) # 处理最开始的start_requests slot = Slot(start_requests, close_if_idle, nextcall, scheduler) self.slot = slot self.spider = spider yield scheduler.open(spider) # 初始化scheduler yield self.scraper.open_spider(spider) # 初始化scraper，这个scraper是处理Request和Item的。 self.crawler.stats.open_spider(spider) # 初始化stats yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider) # 发送爬虫已经打开的通知 slot.nextcall.schedule() # 主动调用_next_request slot.heartbeat.start(5) # 每五秒执行一次_next_request方法 _next_request是核心调度逻辑。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061 def _next_request(self, spider): slot = self.slot if not slot: return# 如果暂停爬虫，就返回 if self.paused: return # 如果spider活着并且没达到并发上线，就寻找下一个请求 while not self._needs_backout(spider): # 把该请求塞入请求队列 if not self._next_request_from_scheduler(spider): break if slot.start_requests and not self._needs_backout(spider): try: # 寻找下一个 start_requests request = next(slot.start_requests) except StopIteration: slot.start_requests = None except Exception: slot.start_requests = None logger.error('Error while obtaining start requests', exc_info=True, extra=&#123;'spider': spider&#125;) else: # 把该请求塞入请求队列 self.crawl(request, spider)# 如果爬虫空闲了，那么就准备退出 if self.spider_is_idle(spider) and slot.close_if_idle: self._spider_idle(spider) def _needs_backout(self, spider): slot = self.slot return not self.running \ or slot.closing \ or self.downloader.needs_backout() \ or self.scraper.slot.needs_backout() def _next_request_from_scheduler(self, spider): slot = self.slot request = slot.scheduler.next_request() if not request: return # 交给downloader去下载请求，添加一堆回调。 d = self._download(request, spider) # 下载后，处理response d.addBoth(self._handle_downloader_output, request, spider) d.addErrback(lambda f: logger.info('Error while handling downloader output', exc_info=failure_to_exc_info(f), extra=&#123;'spider': spider&#125;)) # 移除请求 d.addBoth(lambda _: slot.remove_request(request)) d.addErrback(lambda f: logger.info('Error while removing request from slot', exc_info=failure_to_exc_info(f), extra=&#123;'spider': spider&#125;)) # 继续调用_next_request d.addBoth(lambda _: slot.nextcall.schedule()) d.addErrback(lambda f: logger.info('Error while scheduling new request', exc_info=failure_to_exc_info(f), extra=&#123;'spider': spider&#125;)) return d 1.3 下载器（Downloader） 我们把下载器当做一个更强大的Requests。 它可以轻易地并发，支持http, https,ftp,amazon s3协议。 Downloader干了一件事，将请求分类并且限制请求的并发。 scrapy.scrapy.core.downloader.Downloader 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192class Downloader: def __init__(self, crawler): self.settings = crawler.settings self.signals = crawler.signals self.slots = &#123;&#125; self.active = set() self.handlers = DownloadHandlers(crawler) self.total_concurrency = self.settings.getint('CONCURRENT_REQUESTS') self.domain_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN') self.ip_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_IP') self.randomize_delay = self.settings.getbool('RANDOMIZE_DOWNLOAD_DELAY') self.middleware = DownloaderMiddlewareManager.from_crawler(crawler) # Engine调用这个函数 def fetch(self, request, spider): def _deactivate(response): self.active.remove(request) return response self.active.add(request) # 经过下载中间件，先预处理，然后执行_enqueue_request， 然后做后处理操作。 dfd = self.middleware.download(self._enqueue_request, request, spider) return dfd.addBoth(_deactivate) # 限制并发数 def needs_backout(self): return len(self.active) &gt;= self.total_concurrency def _enqueue_request(self, request, spider): # 获取slot # 这个slot是一个字典，比如&#123;"baidu": 并发1，延迟2秒&#125;，那么百度这个域名就只有1的并发和2的延迟。 key, slot = self._get_slot(request, spider) request.meta['download_slot'] = key def _deactivate(response): slot.active.remove(request) return response slot.active.add(request) self.signals.send_catch_log(signal=signals.request_reached_downloader, request=request, spider=spider) deferred = defer.Deferred().addBoth(_deactivate) slot.queue.append((request, deferred)) self._process_queue(spider, slot) return deferred def _process_queue(self, spider, slot): if slot.latercall and slot.latercall.active(): return # Delay queue processing if a download_delay is configured now = time() delay = slot.download_delay() if delay: penalty = delay - now + slot.lastseen if penalty &gt; 0: slot.latercall = reactor.callLater(penalty, self._process_queue, spider, slot) return while slot.queue and slot.free_transfer_slots() &gt; 0: slot.lastseen = now request, deferred = slot.queue.popleft() dfd = self._download(slot, request, spider) dfd.chainDeferred(deferred) # prevent burst if inter-request delays were configured if delay: self._process_queue(spider, slot) break def _download(self, slot, request, spider): # 真实的下载函数self.handlers.download_request dfd = mustbe_deferred(self.handlers.download_request, request, spider) def _downloaded(response): self.signals.send_catch_log(signal=signals.response_downloaded, response=response, request=request, spider=spider) return response dfd.addCallback(_downloaded) slot.transferring.add(request) def finish_transferring(_): slot.transferring.remove(request) self._process_queue(spider, slot) return _ return dfd.addBoth(finish_transferring) 适当调整并发和下载延迟可以帮助我们反反爬。 1.4 解析器（Scraper） scrapy.scrapy.core.scraper.Scraper对请求和结果进行处理。 关键函数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 处理函数 def _scrape(self, response, request, spider): """Handle the downloaded response or failure through the spider callback/errback""" assert isinstance(response, (Response, Failure)) dfd = self._scrape2(response, request, spider) # returns spiders processed output dfd.addErrback(self.handle_spider_error, request, response, spider) dfd.addCallback(self.handle_spider_output, request, response, spider) return dfd def _scrape2(self, request_result, request, spider): """Handle the different cases of request's result been a Response or a Failure""" if not isinstance(request_result, Failure): # 经过SpiderMiddleware return self.spidermw.scrape_response( self.call_spider, request_result, request, spider) else: # FIXME: don't ignore errors in spider middleware dfd = self.call_spider(request_result, request, spider) return dfd.addErrback( self._log_download_errors, request_result, request, spider)def handle_spider_error(self, _failure, request, response, spider): pass def handle_spider_output(self, result, request, response, spider): if not result: return defer_succeed(None) it = iter_errback(result, self.handle_spider_error, request, response, spider) dfd = parallel(it, self.concurrent_items, self._process_spidermw_output, request, response, spider) return dfd def _process_spidermw_output(self, output, request, response, spider): """Process each Request/Item (given in the output parameter) returned from the given spider """ # 判断yield产生的是Requests还是Item # 如果是请求就继续塞入队列 # 如果是Item就送到Pipeline去处理。 if isinstance(output, Request): self.crawler.engine.crawl(request=output, spider=spider) elif isinstance(output, (BaseItem, dict)): self.slot.itemproc_size += 1 dfd = self.itemproc.process_item(output, spider) dfd.addBoth(self._itemproc_finished, output, response, spider) return dfd elif output is None: pass else: typename = type(output).__name__ logger.error('Spider must return Request, BaseItem, dict or None, ' 'got %(typename)r in %(request)s', &#123;'request': request, 'typename': typename&#125;, extra=&#123;'spider': spider&#125;) 1.5 信号（Signal） 我们可以在各种中间件甚至Spider中添加响应信号的函数。 https://doc.scrapy.org/en/latest/topics/signals.html Scrapy使用了信号来通知某个事件的发生。 总共有以下14种内置在Crawler中的信号。 scrapy.scrapy.signals 1234567891011121314engine_startedengine_stoppedspider_openedspider_idlespider_closedspider_errorrequest_scheduledrequest_droppedrequest_reached_downloaderresponse_receivedresponse_downloadeditem_scrapeditem_droppeditem_error 如果想要在某个signal事件发生的时候调用某个function函数，就用 crawler.signals.connect(, ) 123456789101112131415161718192021222324252627from scrapy import signalsfrom scrapy import Spiderclass DmozSpider(Spider): name = "dmoz" allowed_domains = ["dmoz.org"] start_urls = [ "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/", "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/", ] @classmethod def from_crawler(cls, crawler, *args, **kwargs): spider = super(DmozSpider, cls).from_crawler(crawler, *args, **kwargs) # 当爬虫关闭时，打印log crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed) return spider def spider_closed(self, spider): spider.logger.info('Spider closed: %s', spider.name) def parse(self, response): pass 信号实际是如何运作的？ 那么 我们来自己写一个简单的信号收发demo 12345678910111213141516171819from scrapy.signalmanager import SignalManagerdef receive(): print("Receive signal")if __name__ == '__main__': biubiubiu = object() signal = SignalManager() # biubiubiu发生的时候调用receive函数 signal.connect(receive, signal=biubiubiu) print("Send biubiubiu!") signal.send_catch_log(biubiubiu) # 输出：Send biubiubiu!Receive signal 这里还有个坑，信号的连接函数的生命周期至少要维持到它被调用的一刻，所以不要绑定匿名函数哦~ 来自：https://github.com/scrapy/scrapy/issues/1370 1.6 修改Scrapy 对官方代码的修改，按照这个指南来。 1https://doc.scrapy.org/en/master/contributing.html 1.7 Scrapy生态 用浏览器进行渲染：Splash：https://github.com/scrapy-plugins/scrapy-splash，单CPU大概能到5-6的并发(渲染还是很耗CPU的，这个并发是因为资源请求等待带来的增益)。 分布式部署Scrapy可以使用scrapy-redis队列：https://github.com/rmax/scrapy-redis 可视化爬虫可以参考Portia: https://github.com/scrapinghub/portia 1.8 总结 Scrapy 作为框架级别的存在给我们写爬虫提供了很多便利，但是同样也会带来一些限制。它提供了一套统一的规范，方便我们在写爬虫时复用代码。但是基于Twisted编写的它同样会导致开发人员踩入一些Twisted的坑。Twisted的Deferred机制让新手入门是不习惯的。 事件驱动框架有很多 java: Vert.x Python: Twisted, Tornado, gevent PHP: swoole NodeJS/Chrome JS Backend … 所有的事件驱动框架都是基于 epoll(linux), poll(all)，iocp(windows)来做的。 学习应该从unix网络编程开始。了解到同步异步阻塞非阻塞IO的相关概念，再接触Asyncio跟Twisted，Gevent。在这过程中，会发现gevent的编写简直太方便了。 如果要深入就不用看百度的大部分《异步编程》相关的教程，那些可能都讲的很浅。可以根据事件驱动（event-driven）或者是IO多路复用(io multiplexing)的关键字搜索，协程可以从golang的goroutine中文博客学习。从python学习这些不太好，没什么人讲得深。 接下来，无他唯手熟尔。有兴趣的就可以看源码通关。 2. 反反爬？（Anti-Anti-Crawl） 江湖有爬虫，也必然会有反爬虫。否则数据都被爬取了，服务器的压力是一个方面，数据的价值也是一个方面，还有一方面，尊严。大多数做Web开发的程序猿都不喜欢自己的服务被人爬取，因此会研究一套又一套的反爬措施来针对封杀爬虫。 以下我从针对爬虫的角度来说一些反爬措施。 2.1 简单的反爬 机器人协议 robots.txt有那么点用，可以拒绝正规网站（如搜索引擎）的爬虫。 然而这个协议防君子不防小人，无法阻止个人行为的爬虫。 不过适当写一点robots.txt可以减少很多来自百度、搜狗等搜索引擎的流量。 请求头校验 上图是从Chrome Dev Tool中拿到的某个网页请求的Headers，这就是HTTP请求头的指纹，而来自爬虫的请求往往会缺少或使用错误的部分请求字段（Scrapy, Requests, Curl…）。因此我们就可以在后端进行请求头校验来阻止简单的爬虫。最简单的就是校验User-Agent来过滤只学了2-3天的初学者爬虫。 IP封锁 正常用户一般来说，会隔n秒发一个请求（n由用户的电脑和网络性能决定）。假如这个用户心情不好（分手/失恋/被甩），那么可能会疯狂刷新来表达自己的失落。 所以一秒能发好几次请求的爬虫肯定就是爬虫，那么我们可以设置一个阈值来针对这些爬虫了。 但是IP其实不太值钱，几百块钱就可以拥有几千个不重复的IP。 IP封锁可以很好地针对一些初级爬虫。 2.2 难的反爬 模拟登录 接口会验证登录的信息，那么爬虫首先需要进行模拟登录，拿到登录后的cookie。 我们可以在登录上做点手脚。 隐藏表单 在表单提交的时候会同时提交这些hidden的字段，如果没有这些字段，那么表单提交请求就会被认为是爬虫。 当然爬虫仍然可以通过xpath/selector的手段拿到这些隐含字段，附加到表单中就可以绕过了。 验证码 图形验证码 点击验证码 我又双叒叕需要点击验证码了。 滑动验证码 这些说简单也简单，说难也难。 自己弄OCR，深度学习 或者用第三方的服务。 再不行就人工打码平台，都能解决。 就是对于爬虫来说，成本更加高昂了。 登录方式 扫码登录。 手机验证码登录。 对于爬虫来说登录难度就更大了。 JavaScript Iframe 把展示的内容放在iframe中。 Ajax 这个不算爬虫，ajax数据就是前后端分离的一种数据交互方式。当然可以返回一部分处理前的数据，让js再处理一下这部分数据。小网站这种接口一般不会有验证。 参数加密，token验证 混淆一下js文件，用一些混淆工具吧，压缩格式，替换变量名，等等。 前端根据文档写好加密算法的js实现生成token。再隐藏一下入口，多段验证。甚至这个加密算法可以调参，每次发布的时候换一个算法，甚至换一个文件，让做爬虫的加班到gg吧。 对js不熟悉的爬虫开发者来说，会看不懂的。 对js比较熟悉的爬虫开发者来说，也会被混淆搞得很难受。 React,Vue 静态内容全部在压缩的app里。这个react前端中所有数据都是ajax获取的。 vue也是类似的原理。 单页面应用(single page web application，spa)大概都是这么做的。 不过也有“开倒车”的，主动用服务端渲染(server side render, ssr)的，方便百度、谷歌收录的同时也方便了爬虫开发者。 浏览器 很多爬虫开发者经历了前面那些反爬的措施后，就会感觉爬虫比较难做。 爬虫还有一个杀器就是浏览器。 比如90%文章会用Selenium进行模拟登录。 还有的用Splash(Scrapy出的)。基于PyQT5的webkit包了一层好用的HTTP接口。 PhantomJS(2017年4月Chromium推有了Headless模式后PhantomJS作者就宣布我不干了，以后Chromium会养你们的) Chrome团队也推了一个https://github.com/GoogleChrome/puppeteer 想要取代selenium做自动化的测试工具。至此大概明白了，前端的将来的爸爸是Chrome 而不是IE或者FireFox。 （欢迎使用Chrome全家桶） 爬虫开发者发现这些神器，欣喜若狂， 本来费尽心思模拟浏览器行为欺骗服务器。现在直接提供一个浏览器，只需要写个脚本来模拟用户在页面中行为，相当于修改一下dom就可以拿到各种数据了，岂不美哉？ 于是就跟开头说的一样，一大堆用Selenium的文章，本来请求分析要2-5小时才能模拟登录，现在用Selenium直接模拟登录。也不用管ajax，直接想模拟点击就模拟点击，想下拉就下拉，想跳转页面就直接跳转，然后xpath/selector就拿到了一堆数据。 Selenium巨慢，可以用多进程来启动多个增加并发量。 以为Selenium就是银弹了吗，那当然不可能。大公司反爬虫部门也做了一些事。使用Selenium的webdriver会发现有一些特征是和网民正常上网区分开来的。比如我们可以写一段简单的代码比较主流浏览器跟那些webdriver驱动的浏览器有哪些区别，根据这些区别编写js文件来打击那些爬虫开发。 最最基础的，根据w3c规范，在使用webdriver时，window.navigator.webdriver需要设置为true 参考：https://stackoverflow.com/questions/33225947/can-a-website-detect-when-you-are-using-selenium-with-chromedriver 12345678//我们可以在登录前执行() =&gt;&#123;Object.defineProperties(navigator,&#123; webdriver:&#123; get: () =&gt; false &#125; &#125;)&#125; 这个只是webdriver的n个特征中的一个， 很多时候我们需要对webdriver进行一些注入操作，来使得它通过 webdriver/无头浏览器的特征检测, 推荐看一下这篇绕过无头浏览器的文章：https://intoli.com/blog/not-possible-to-block-chrome-headless/ 如何注入JS：https://github.com/intoli/intoli-article-materials/tree/master/articles/javascript-injection， 这时候已经能搞定99.9%网站的浏览器指纹检查了，包括淘宝这样的网站。 用Splash也是另一种思路，用Splash能够绕过一些Chrome Headless的活。 然而淘宝还有判断常用地登录的一套机制，如果你不是常用地登录的话就会检查到，从而进入淘宝风控机制。 淘宝最方便的还是通过扫码登录，然后利用xposed去扫这个码。 所有这些具体实现按照自己习惯就可以。 2.3 APP反爬 抓取HTTP流量？ 没事我们还有请求参数token校验。怎么破解？反编译？利用自动化测试工具Appium？ 想抓取HTTPS流量？ 使用证书锁定技术（APP使用自己的证书）。因此Fiddler/Charles的证书是无效的，不能抓取到APP的流量。 参考：https://stackoverflow.com/questions/33382870/how-to-capture-httpstls-1-0-communications-from-android-app-with-fiddler4 2.4 投毒/蜜罐 识别爬虫的办法很多，我们能确认用户不会做的行为，那都会是爬虫做的。 在识破了爬虫后，我们就可以采取一些行为制裁一下这些爬虫了。 电商网站给假商品假价格 404不存在 无限302跳转 一段爱的感化，比如：爬海无涯，回头是前端开发 当然最简单的还是直接拒绝掉，节省服务器资源。 2.5 最终打击 技术不够，法律来凑。遇到这种情况了，就假装自己不会写爬虫吧。 2.6 总结 总而言之，反爬就是把一切不像网民行为的请求给ban掉，反反爬就是让自己的请求像一个正常网民的行为来欺骗网站。 反爬和反反爬，一个在暗一个在明，江湖还将继续延续下去。。。 3 相关书籍 Learning Scrapy twisted异步编程入门 4. Reference]]></content>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Twisted中遇到的事件驱动框架脏读问题]]></title>
    <url>%2F2018%2F08%2F09%2FScrapy%2F%E4%BA%8B%E4%BB%B6%E9%A9%B1%E5%8A%A8%E6%A1%86%E6%9E%B6%E8%84%8F%E8%AF%BB%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Twisted中遇到的事件驱动框架脏读问题 在进行Scrapy中间件开发时遇到了全局变量被修改的问题。类比线程有一个线程安全问题， 协程也有协程安全的问题需要考虑。 为什么会有&quot;线程安全&quot;的问题出现呢？ 123456789def parallel(iterable, count, callable, *args, **named): """Execute a callable over the objects in the given iterable, in parallel, using no more than ``count`` concurrent calls. Taken from: https://jcalderone.livejournal.com/24285.html """ coop = task.Cooperator() work = (callable(elem, *args, **named) for elem in iterable) return defer.DeferredList([coop.coiterate(work) for _ in range(count)]) 并行函数是通过Cooperator实现的， 这个Cooperator的特点就是，同时添加多个任务， 一个Cooperator有一个定时器Timer来控制执行时间最多为10ms。也就是说，一个包含了100个item的Cooperator，一次理论上执行最多10ms后就得到下一个周期再执行了。 当这个Cooperator执行10毫秒后， Twisted框架自动跳出这个Cooperator切换到下一个要执行的任务（函数），如果之后的某一个函数修改了公共变量（全局变量或者类变量）， 再切回该Cooperator去访问该公共变量，这时候读到的值就发生了变化。这里的&quot;线程安全&quot;就是脏读现象。 正常情况下会遇到吗？ Scrapy使用Cooperator的本意是为了限制item的并发数，防止执行item相关的pipeline耗时过久。 Scrapy的yield并不能简单地理解为 Python的协程， 而是经过Scrapy自己的一层包装后再递交给Twisted。 Cooperator 非常特殊， Twisted会强制限制执行时间进行切换。 一般情况下会执行完一个协程后再切换，而Cooperator包了一堆Deferred，以Deferred为单位切换，相当于限制了一个协程执行的时间， 一个协程执行到一半就切换到了下一个协程去。 理论上像Deferred，Future，Promise都不会遇到这种问题。 他们并不是直接进行IO切换，而是先加入一个schedule队列再执行完剩余的code。 Gevent跟其他的事件驱动框架不同，在遇到IO阻塞的情况下会直接切换协程， 这时候就容易出现脏读问题。 结论： gevent会遇到也比较容易遇到这个问题， 其它事件驱动框架如果提供了 强制切换协程的API，也会遇到这个问题。同时我们在Scrapy开发中间件时不要随意改变全局变量并使用，用局部绑定变量处理。 本地复现（Twisted实现） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import timeimport randomfrom twisted.internet import defer, reactor, taskclass T(): def __init__(self): self.b = 5t = T()t.b = 1def _(x): parallel(get_urls(), 100, download) print('tb', t.b)def get_urls(): t.b = 2 time.sleep(0.01) yield 'url1' time.sleep(0.01) t.b = 1def change_tb(): t.b = 3def download(url): # 类似yield request request = defer.Deferred() # request.addCallback(lambda x: change_tb()) request.addCallback(lambda x: parallel(get_urls(), 100, download)) reactor.callLater(random.random(), request.callback, url) # 类似yield item item = defer.Deferred() item.addCallback(_) reactor.callLater(0, item.callback, url) return itemdef parallel(iterable, count, callable, *args, **named): coop = task.Cooperator() work = (callable(elem, *args, **named) for elem in iterable) return defer.DeferredList([coop.coiterate(work) for _ in range(count)])urls = get_urls()for i in range(1): finished = parallel(get_urls(), 100, download)reactor.run()]]></content>
      <categories>
        <category>twisted</category>
      </categories>
      <tags>
        <tag>event-driven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编码风格规范]]></title>
    <url>%2F2018%2F07%2F30%2FPythonCodeStyle%2FPython%E7%BC%96%E7%A0%81%E9%A3%8E%E6%A0%BC%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[Python编程规范 1 背景 ​ Python是一门动态语言。Python有着非常自由的语法和非常多的特性，开发者可以写出各种充满社会特色主义的代码。本规范的目的不是为了约束开发者的创造力和生产力，而是为了限制开发者的过度个性化，统一团队的代码风格，通过风格一致的代码来更好地进行团队成员之间的协作开发。本规范适用Python3.5以上的版本，部分规范在Python2.7不一定适用。本规范也不推荐开发者继续使用Python2.7开发，Python3会不断地加入新的特性，而Python2.7 到2020年1月1日后将停止被维护。本规范将辅助开发者写出简洁、优雅、高效和风格一致的代码，并且减少踩坑的概率，提升代码的质量，增强开发者的编程规范意识。 ​ 本规范主要参照google code style和 PEP-8完成。 2 语言规范 2.1 代码检查工具 flake8 IDE自带代码检查工具 2.1.1 解释 代码检查工具可以找出代码中存在的明显的bug和风格问题。 2.1.2 优点 可以找出容易忽略的错误，比如单词拼写错误， 使用了未被赋值的变量等。 2.1.3 缺点 代码检查工具不是万能的， 有些warning可能是不对的，在必要的时候可以忽略它们。 2.1.4 推荐 确保你写的代码经过了代码检查工具的检查，并忽略不必要的warning。 2.2 导入(Imports) 使用import来导入。 2.2.1 解释 模块间的代码复用。 2.2.2 优点 命名空间管理约定简单。x.Obj表示Obj对象在模块x中。 2.2.3 缺点 绝对导入在嵌套和命名复杂的情况下， 导入代码会变得非常长。 2.2.4 推荐 使用import x来导入包和模块。 使用from x import y当x是包名， y是模块名，模块名最好不要包含前缀。 使用from x import y as z,如果y已经被导入了，为了避免重名，重命名为z。 使用from x.y import a, b, c x是包名,y是模块名， a,b,c是类，方法或者变量名。 使用import y as z，只有业界都这么做的时候才这么做。(比如import numpy as np， 是通用的一个做法) 避免方法内的局部导入。(除非遇到循环引用的情况，这个时候需要检查是否必须只能循环引用，能不能拆分代码避免循环引用) ​ 2.3 包(Packages) 使用模块的绝对路径名来导入每个模块。 2.3.1 解释 包是按目录来组织模块的方法。 2.3.2 优点 避免模块名冲突，查找模块更加容易。 2.3.3 缺点 导入变得困难， 因为必须要带上整个包的路径。 2.3.4 推荐 使用绝对导入，不推荐使用相对导入。 12345# Reference in code with complete name.import absl.flags# Reference in code with just module name (这种方法更好).from absl import flags 2.4 异常(Exceptions) 异常需要小心使用。 2.4.1 解释 异常是一种跳出代码块的正常控制流来处理错误或者其他异常情况的方式。 2.4.2 优点 正常操作代码的控制流不会和处理错误的代码混合在一起。当某个特定异常发生的时候，控制流可以跳过。比如，一步跳出N个嵌套的函数，而不用继续执行错误的代码。 2.4.3 缺点 控制流可能会让人疑惑。在调用库的时候容易忽略一些错误情况。 2.4.4 推荐 使用raise MyError('Error message') 或 raise MyError() 充分利用内置的异常类。比如，如果你传递了一个负数，但是要求是一个正数，抛出一个ValueError。不要使用assert语句来验证公共API中参数的值。assert被用于确保内部的正确性，而不是为了强制正确的使用，也不是为了表明一些意外的事情发生。如果发生了意外，请使用raise语句。比如， 12345678910Yes: def ConnectToNextPort(self, minimum): """Connects to the next available port. Returns the new minimum port.""" if minimum &lt;= 1024: raise ValueError('Minimum port must be greater than 1024.') port = self._FindNextOpenPort(minimum) if not port: raise ConnectionError('Could not connect to service on %d or higher.' % (minimum,)) assert port &gt;= minimum, 'Unexpected port %d when minimum was %d.' % (port, minimum) return port 1234567No: def ConnectToNextPort(self, minimum): """Connects to the next available port. Returns the new minimum port.""" assert minimum &gt; 1024, 'Minimum port must be greater than 1024.' port = self._FindNextOpenPort(minimum) assert port is not None return port 库和包可以定义它们自己的异常。如果要这么做， 必须继承一个已有的异常类。异常名需要以Error结尾， 永远不要使用 except: 语句来捕获所有异常， 该语句将会捕获SystemExit 和KeyboardInterrupt异常，这样会很难通过Control-C中断程序， 并且会掩盖掉其它的异常。如果想要捕捉程序出错的异常，使用except Exception as e:。 尽量减少try/except块内的代码量。try的内容越多， 期望外的异常就越容易被触发。在这种情况下try/except块将隐藏真正的错误。 使用finally子句。无论有没有异常被抛出，finally子句内的代码都会被执行。对于需要执行清理操作的时候，这是非常有用的。比如，关闭一个文件。 当捕获异常时，使用as而不要使用逗号(python3不支持逗号)。比如： 1234try: raise Errorexcept Error as error: pass ​ 2.5 全局变量(Global variables) 避免全局变量 2.5.1 解释 模块级的变量或类变量 2.5.2 优点 偶尔有用。 2.5.3 缺点 导入时可能会改变模块行为，因为在首次导入模块时会对全局变量赋值。 2.5.4 推荐 避免使用全局变量。 例外： 模块级常量。比如，MAX_HOLY_HANDGRENADE_COUNT = 3。常量单词需要全部大写并且用下划线连接。具体请查看命名规范。 如果需要，全局变量仅在模块内部可用，并在名字前加一个_。外部访问必须通过公共模块级函数来完成。 2.6 嵌套、局部、内部类和函数(Nested/Local/Inner Classes and Functions) 当嵌套的局部方法或者类是允许的。 2.6.1 解释 一个类可以在方法、函数或类中被定义。一个函数可以在方法或函数中被定义。嵌套方法拥有对闭包变量只读的权限。 2.6.2 优点 允许写一点小的工具类和函数在局部(用于辅助一些操作)，方便直接调用。 比如： 123456789101112import sysdef Foo(): def e(s): sys.stderr.write('ERROR: ') sys.stderr.write(s) sys.stderr.write('\n') e('I regret to inform you') e('that a shameful thing has happened.') e('Thus, I must issue this desultory message') e('across numerous lines.')Foo() 2.6.3 缺点 嵌套和局部的类实例不能被序列化(pickled)。嵌套的方法和类不能被直接得测试。嵌套可能会让你的函数更长并减少可读性。 2.6.4 推荐 推荐使用。 2.7 推导式和生成器表达式(Comprehensions &amp; Generator Expressions) 适合在简单情况下使用。 2.7.1 解释 列表，字典和集合推导式和生成器表达式提供了一种简洁高效的方式来创造容器类和迭代器，而不必借助于传统的循环， map(), filter(), 或者是lambada。 2.7.2 优点 简单的推导式比其他dict, list或set的创建方法更加清晰简单。生成器表达式非常高效， 因为它们避免一次性创建整个列表。 2.7.3 缺点 复杂的推导式或生成器表达式会导致阅读障碍。 2.7.4 推荐 适合在简单情况下使用。 每个部分应该单独一行：映射(mapping)表达式， for语句， 过滤表达式。 禁止多重for语句或者过滤表达式。在复杂情况下，使用循环。 12345678910111213141516171819202122Yes: result = [] for x in range(10): for y in range(5): if x * y &gt; 10: result.append((x, y)) for x in xrange(5): for y in xrange(5): if x != y: for z in xrange(5): if y != z: yield (x, y, z) return ((x, complicated_transform(x)) for x in long_generator_function(parameter) if x is not None) squares = [x * x for x in range(10)] eat(jelly_bean for jelly_bean in jelly_beans if jelly_bean.color == 'black') 123456789No: result = [(x, y) for x in range(10) for y in range(5) if x * y &gt; 10] return ((x, y, z) for x in xrange(5) for y in xrange(5) if x != y for z in xrange(5) if y != z) 2.8 默认迭代器和操作符(Default Iterators and Operators) 如果类支持默认的迭代器和操作符，那么就使用它们，比如列表，字典和文件。 2.8.1 解释 字典和容器类型，比如字典和列表，定义了默认的迭代器和成员关系操作符(in和not in) 2.8.2 优点 默认迭代器和操作符简单高效。它们直接调用相关操作，不会因为额外的调用方法而产生开销。使用默认操作符的函数是通用的。任何类都可以去选择实现相关方法来支持这些操作。 2.8.3 缺点 类似in和not in 无法直接判断该对象的类型，而如果使用 has_key()则意味着该对象是一个字典。 2.8.4 推荐 如果默认的类支持默认的迭代器和操作符，比如列表，字典和文件。内建容器类型也定义了迭代器方法。优先考虑这些方法，而不是用另外的返回列表的方法。请注意，在迭代容器的时候，你是不能对它进行修改的。 12345Yes: for key in adict: ... if key not in adict: ... if obj in alist: ... for line in afile: ... for k, v in dict.iteritems(): ... 123No: for key in adict.keys(): ... if not adict.has_key(key): ... for line in afile.readlines(): ... 2.9 生成器(Generators) 按需使用生成器。 2.9.1 解释 一个生成器函数返回一个迭代器。每当它执行一次yield语句，它就会返回一个值。生成值后，生成器函数的运行状态被挂起直到需要生成下一个值。 2.9.2 优点 简化代码， 因为每次调用时，局部变量和控制流的状态都会被保存。生成器仅会占用极少的内存，对于大内存对象的迭代，推荐使用生成器而不是列表。 2.9.3 缺点 无。 2.9.4 推荐 鼓励使用。 2.10 匿名函数(Lambda Functions) 适用于一行的表达式。 2.10.1 解释 Lambda定义了以一个表达式作为匿名函数，经常用于回调和高阶函数比如map()，filter()。 2.10.2 优点 方便。 2.10.3 缺点 比本地函数更难阅读和调试。没有函数名意味着调用栈跟踪更加难以理解。由于python的lambda函数只能包含一个表达式，因此它的作用有限。 2.10.4 推荐 对于简单的表达式，可以直接使用匿名函数而不用再单独写一个函数。如果匿名函数代码超过60-80个字符，最好还是定义成单独的函数。对于常见的操作符，比如乘法，使用operator模块中的函数代替lambda函数。比如， 推荐使用operator.mul， 而不是lambda x,y: x*y。 2.11 条件表达式(Conditional Expressions) 2.11.1 解释 条件表达式(又被称为三元运算符)提供了比 if语句更短的语法。比如x = 1 if cond else 2。 2.11.2 优点 比if语句更加简短和方便。 2.11.3 缺点 比if语句读起来累，特别是一个表达式很长的时候。 2.11.4 推荐 适用于简单的，一行能解决的情况。在需要多行语句才能解决的情况，推荐使用完整的if语句。 2.12 默认参数值(Default Argument Values) 适用于大部分情况。 2.12.1 解释 函数的参数可以指定默认值。如def foo(a, b=0):。如果foo只传入一个参数，那么b的值就是0。如果传入两个参数，b的值就是第二个参数的值。 2.12.2 优点 你经常会碰到一些使用大量默认值的函数， 但偶尔(比较少见)你想要覆盖这些默认值。默认参数值提供了一种简单的方法来完成这件事， 你不需要为这些罕见的例外定义大量函数。同时, Python也不支持重载方法和函数，默认参数是一种&quot;仿造&quot;重载行为的简单方式。 2.12.3 缺点 默认参数只在模块加载时求值一次，如果参数是列表或字典之类的可变类型, 这可能会导致问题。如果在函数内部对该可变对象进行修改(比如向列表追加项l.append('sth'))，默认参数的值就被修改了。 2.12.4 推荐 鼓励使用。不过有如下注意事项： 不要在函数或方法定义中使用可变对象作为默认值。 123456Yes: def foo(a, b=None): if b is None: b = []Yes: def foo(a, b: Optional[Sequence] = None): if b is None: b = [] 123456No: def foo(a, b=[]): ...No: def foo(a, b=time.time()): # The time the module was loaded??? ...No: def foo(a, b=FLAGS.my_thing): # sys.argv has not yet been parsed... ... 2.13 属性(Properties) 访问和设置数据成员时， 你通常会使用简单，轻量级的访问和设置函数。建议用属性(properties)来代替它们。 2.13.1 解释 能够包装getter和setter。 当运算量不大, 它是获取和设置属性(attribute)的标准方式. 2.13.2 优点 指定显式的属性get和set方法可以提高可读性。允许延迟计算(lazy calculation)。不需要对类的接口进行改动。 2.13.3 缺点 在实例属性需要频繁访问的时候， 通过直接访问属性的速度会是通过该方式访问属性的3倍。在高性能编程下需要注意@property可能带来的性能问题。 2.13.4 推荐 推荐使用@property装饰器的方式来获取或者设置属性，在获取属性需要通过简单的计算表达式得出结果时，这是非常适合的。比如： 1234Yes:@propertydef area(self): return math.pi * self.radius**2 123456789101112131415161718192021222324252627282930313233343536373839404142Yes: import math class Square(object): """A square with two properties: a writable area and a read-only perimeter. To use: &gt;&gt;&gt; sq = Square(3) &gt;&gt;&gt; sq.area 9 &gt;&gt;&gt; sq.perimeter 12 &gt;&gt;&gt; sq.area = 16 &gt;&gt;&gt; sq.side 4 &gt;&gt;&gt; sq.perimeter 16 """ def __init__(self, side): self.side = side @property def area(self): """Gets or sets the area of the square.""" return self._get_area() @area.setter def area(self, area): return self._set_area(area) def _get_area(self): """Indirect accessor to calculate the 'area' property.""" return self.side ** 2 def _set_area(self, area): """Indirect setter to set the 'area' property.""" self.side = math.sqrt(area) @property def perimeter(self): return self.side * 4 避免太过简单的包装。 12345No: @property def area(self): """Gets or sets the area of the square.""" return self._area 避免通过property的方式设置getter和setter。使用@property和@sth.setter。 123No:area = property(___get_area, ___set_area, doc="""Gets or sets the area of the square.""") 2.14 True/False的求值(True/False evaluations) 尽可能使用隐式false 2.14.1 解释 Python在布尔上下文中会将某些值求值为False。 按简单的直觉来讲， 就是所有的&quot;空&quot;值都被认为是false。因此 0, None, [], {}, &quot;&quot;都被认为是false值。 2.14.2 优点 使用Pythonic布尔值的条件语句更易读也更不易犯错。 2.14.3 缺点 对于C/C++等其它语言开发人员来说，会感觉有点奇怪。 2.14.4 推荐 尽可能使用隐式的false， 例如: 使用 if foo: 而不是 if foo != []: . 不过还是有一些注意事项需要你铭记在心: 永远不要用==或者!=来比较某些单例， 比如None。 使用is或者is not。 注意: 当你写下 if x: 时， 如果你其实表示的是 if x is not None，那么if x:是不对的。 例如: 当你要测试一个默认值是None的变量或参数是否被设为其它值。 这个值在布尔语义下可能是false！ 永远不要用==将一个布尔量与false相比较. 使用 if not x: 代替. 如果你需要区分false和None, 你应该用像 if not x and x is not None: 这样的语句。 对于序列(字符串, 列表, 元组), 要注意空序列是false。因此 if not seq: 或者 if seq: 比 if len(seq): 或 if not len(seq): 要更好。 处理整数时, 使用隐式false可能会得不偿失(即不小心将None当做0来处理). 你可以将一个已知是整型(且不是len()的返回结果)的值与0比较。 123456789101112Yes: if not users: print('no users') if foo == 0: self.handle_zero() if i % 10 == 0: self.handle_multiple_of_ten() def f(x=None): if x is None: x = [] 1234567891011No: if len(users) == 0: print('no users') if foo is not None and not foo: self.handle_zero() if not i % 10: self.handle_multiple_of_ten() def f(x=None): x = x or [] 注意’0’(字符串)会被当做true。 ​ 2.15 过时的语言特性(Deprecated Language Features) 使用字符串方法代替字符串模块。使用列表推导式和for循环代替filter。在map 中少用复杂的lambda函数。使用for循环而不是reduce。 2.15.1 解释 当前版本的Python提供了大家更喜欢的特性。 2.15.2 推荐 我们不使用陈旧的Python版本，而是尽可能使用新的和稳定的Python版本。所以我们应该尝试、习惯并探索更好的特性。 1234567Yes: words = foo.split(':') [x[1] for x in my_list if x[2] == 5] map(math.sqrt, data) # Ok. No inlined lambda expression. fn(*args, **kwargs) 12345No: words = string.split(foo, ':') map(lambda x: x[1], filter(lambda x: x[2] == 5, my_list)) apply(fn, args, kwargs) 2.16 变量作用域(Lexical Scoping) 不要使用会让人误解的局部变量和全局变量，不同的变量作用域不同。 2.16.1 解释 嵌套的Python函数可以引用外层函数中定义的变量，但是不能够对它们赋值。变量绑定的解析是使用词法作用域, 也就是基于静态的程序文本。对一个块中的某个名称的任何赋值都会导致Python将对该名称的全部引用当做局部变量， 甚至是赋值前的处理。 如果碰到global声明，该变量就会被视作全局变量。 一个使用闭包的例子： 123456def get_adder(summand1): """Returns a function that adds numbers to a given number.""" def adder(summand2): return summand1 + summand2 return adder 调用方式sum = get_adder(summand1)(summand2) 2.16.2 优点 结果更加清晰，对Lisp和Scheme(Haskell，ML等)程序员友好 2.16.3 缺点 可能会产生令人费解的bug。 例如这个依据 PEP 0227 的例子: 12345678910i = 4def foo(x): def bar(): print(i, end='') # ... # A bunch of code here # ... for i in x: # Ah, i *is* local to foo, so this is what bar sees print(i, end='') bar() 因此 foo([1, 2, 3]) 会打印 1 2 3 3 ，不是 1 2 3 4 。 因为foo函数中的i被隐式赋值为3， 而不是使用全局的4。 2.16.4 推荐 非常熟悉该特性的开发者可以使用。否则慎用。 闭包会有一些问题比如： 12345678910No:def count(): fs = [] for i in range(1, 4): def f(): return i*i fs.append(f) return fsf1, f2, f3 = count() 你可能认为调用f1()，f2()和f3()结果应该是1，4，9，但实际结果是： 123456&gt;&gt;&gt; f1()9&gt;&gt;&gt; f2()9&gt;&gt;&gt; f3()9 解释：返回函数引用变量i，但是它并不是立即执行的，而是延迟执行。等到3个函数都返回时，它们所引用的变量i已经变成了3，因此最终结果为9。 如果一定要引用循环变量怎么办？方法是再创建一个函数立即执行，用该函数的参数绑定循环变量当前的值，无论该循环变量后续如何更改，已绑定到函数参数的值不变： 12345678910Yes:def count(): def f(j): def g(): return j*j return g fs = [] for i in range(1, 4): fs.append(f(i)) # f(i)立刻被执行，因此i的当前值被传入f() return fs 1234567&gt;&gt;&gt; f1, f2, f3 = count()&gt;&gt;&gt; f1()1&gt;&gt;&gt; f2()4&gt;&gt;&gt; f3()9 返回闭包时牢记：返回函数不要引用任何循环变量，或者后续会发生变化的变量。 2.17 函数和方法装饰器(Function and Method Decorators) 使用装饰器。避免使用@staticmethod以及限制使用@classmethod 2.17.1 解释 用于函数及方法的装饰器 (也就是@标记)。一个流行的装饰器是@property用于把普通的方法转化成属性。装饰器也允许由用户自定义。比如my_decorator： 1234class C: @my_decorator def method(self): # method body ... 等同于 1234class C: def Methodmethod(self): # method body ... Methodmethod = MyDecoratormy_decorator(Methodmethod) 2.17.2 优点 优雅地定义了，减少重复的代码，从纵向上减少代码的耦合程度。具有面向切面编程的优点。 2.17.3 缺点 装饰器会修改函数的参数或者返回值，这些操作都是隐式的。另外装饰器在导入时(import time)与函数绑定生效。装饰器中产生的错误会更加难追查。 2.17.4 推荐 如果有非常明显的优点(代码复用，面向切面编程)，那么就可以使用它。装饰器需要遵守和函数一样的导入和命名规则。装饰器的文档需要说明这是一个装饰器。为装饰器编写健壮的单元测试。 避免装饰器对外部的依赖(比如不要依赖文件，socket，数据库连接等)。要保证装饰器的有效调用在任何情况下都是成功的。 装饰器是一种特殊的“顶层代码”(top level code)。 少用@staticmethod。大多数情况下，直接使用模块级的函数更加适合而不是跟类耦合在一起。少部分情况下，在函数只被该类使用的情况下，可以使用@staticmethod。 仅在写构造函数时使用@classmethod。如果要修改类变量的值，也需要使用@classmethod。 2.18 线程(Threading) 不要依赖内建类型(built-in)的原子性。 2.18.1 推荐 虽然Python的内建类型比如字典的操作看上去都是原子的，但是在某些情况下它们仍然不是原子的(如果__hash__或__eq__以Python代码实现)，因此他们的原子性是不可靠的。 使用Queue作为线程间的数据通信方式。其它情况，使用threading模块和锁原语(locking primitives)。学习condition的适用场景，你可以使用threading.Condition来取代低级别的Lock。 2.19 强大的特性(Power Features) 强大不等于好用， 相反强大的特性正式因为它能做的事情过于强大，容易让初中级程序员翻船。 2.19.1 解释 Python是非常灵活的语言， 它为你提供了很多花哨的特性，诸如元类(metaclasses)，字节码(bytecode)访问, 任意编译(on-the-fly compilation)，动态继承(dynamic inheritance)，对象父类重定义(object reparenting)，导入黑客(import hacks), 反射，系统内修改(modification of system internals)等等。 2.19.2 优点 强大的语言特性，可以让你的代码更加简洁紧凑。 2.19.3 缺点 奇淫技巧的代码会变得难以阅读和调试。刚开始写下这些代码时还能理解，但是当后续回顾这些代码时，理解起来还不如一些虽然写得比较长还是表达意思很直接的代码。 2.19.4 推荐 除非你对你将要使用的特性理解非常深入(你能够理解为什么要这么做，以及懂得怎么正确地去做)， 否则不要使用这些奇淫技巧。 有些特性经过封装后，我们认为它是相对安全并且容易理解的。 部分使用了这些特性的标准库的模块和类是允许使用的(比如abc.ABCMeta，collections.namedtuple和enum)。 2.20 现代化Python(Modern Python: Python 3 and from _future_ imports) Python3即将开启新的时代！ 但是某些项目还不支持Python3， 因此那些项目需要进行迁移或是兼容。 2.20.1 解释 Python3是Python的一个重要的改变。但是Python2的代码仍然存在，这时候需要利用future模块来进行兼容，而不需要做太大的改动。 2.20.2 优点 Python3的语法更加优雅。 2.20.3 缺点 future模块可能有些丑。没错，毕竟future是为了兼容所做的妥协。 2.20.4 推荐 123from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_function 如果你对这些兼容为什么要这么做不太了解，你可以阅读 absolute imports, new / division behavior, and the print function。 future, six模块可以帮助你进行Python2和3的兼容。 如果没有兼容Python2的必要，请选择Python3.5+的版本进行编程。 仔细阅读Python2到3迁移，并对自己的项目进行改造。😄 在100%的情况下不建议再使用Python2进行编程。 2.21 代码类型注释(Type Annotated Code) 非常好的特性， 你可以对Python3.5+的代码进行注释，参考 PEP-484。从python3.6开始还增加了变量注释，参考PEP-0526。 2.21.1 解释 类型注释type annotations(或者说类型提示type hints)是对函数参数和返回值进行注释。 类型注释： 1def func(a: int) -&gt; List[int]: 变量注释： some_number: int valuesome_list: List[int] = [] 2.21.2 优点 类型注释提高了你代码的可读性和可维护性。类型检查器可以根据类型注释把可能在运行时发生的错误提前检查出来。提高了项目的工程强度。 2.21.3 缺点 Python3.5+支持。你需要不停更新这些类型声明。但是这会让你对你的代码非常熟悉，其实也算是一个优点。 可能会限制开发者使用一些Python奇淫技巧的能力。 2.21.4 推荐 如果你的项目复杂到你必须进行类型注释，那么这一定是一个值得一试的特性。 把代码变成类似静态语言的风格并不是在开倒车，在一个大型项目下这是必须要做的事情。 3 Python风格规范 3.1 代码格式化工具 使用代码格式化插件格式化代码。 Pycharm 使用Cmd + Option + L（Windows：Control+Alt+L）快捷键进行格式化。 Visual Studio Code 通过pip安装flake8和yapf 安装VSCode的Python扩展 使用Option+Shift+F（Windows：Alt+Shift+F） 3.2 分号(Semicolons) 不要在行尾添加分号，也不要用分号将两条命令放在同一行 123Yes:import aimport b 12No:import a;import b; 3.3 行长度(Line length) 每行不超过120个字符。 例外： 很长的导入语句。 URL，路径名，或者注释中非常长的flag。 不要使用反斜杠换行。 Python会把(),[],{}中的行自动隐式地连接到一起， 可以利用这个特性连接行。 12345Yes: foo_bar(self, width, height, color='black', design=None, x='foo', emphasis=None, highlight=0) if (width == 0 and height == 0 and color == 'red' and emphasis == 'strong'): 如果一个字符串在一行放不下， 使用圆括号来进行隐式的连接： 12x = ('This will build a very long long ' 'long long long long long long string') 在注释中，可以把长的URL放在一行上。 12Yes: # See details at # http://www.example.com/us/developer/documentation/api/content/v2.0/csv_file_name_extension_full_specification.html 123No: # See details at # http://www.example.com/us/developer/documentation/api/content/\ # v2.0/csv_file_name_extension_full_specification.html 在使用with语句时， 三个以上的表达式可以被反斜杠分为多行。对于两行表达式，使用嵌套with。 1234Yes: with very_long_first_expression_function() as spam, \ very_long_second_expression_function() as beans, \ third_thing() as eggs: place_order(eggs, beans, spam, beans) 123No: with VeryLongFirstExpressionFunction() as spam, \ VeryLongSecondExpressionFunction() as beans: PlaceOrder(eggs, beans, spam, beans) 123Yes: with very_long_first_expression_function() as spam: with very_long_second_expression_function() as beans: place_order(beans, spam) 3.4 括号(Parentheses) 少用括号。 除非是为了实现行与行之间的连接，否则不要在返回语句或条件语句中使用括号。在元祖两边使用括号是必要的。 1234567891011121314Yes: if foo: bar() while x: x = bar() if x and y: bar() if not x: bar() # 对于只包含一项的元祖，比起onesie = foo来说, 加上括号更加明显 onesie = (foo,) return foo return spam, beans return (spam, beans) for (x, y) in dict.items(): ... 12345No: if (x): bar() if not(x): bar() return (foo) 3.5 缩进(Indentation) 使用4个空格缩进代码。 不要使用tab！对于行链接的情况，使用垂直对齐： 123456789101112131415161718192021222324252627Yes: # Aligned with opening delimiter foo = long_function_name(var_one, var_two, var_three, var_four) meal = (spam, beans) # Aligned with opening delimiter in a dictionary foo = &#123; long_dictionary_key: value1 + value2, ... &#125; # 4-space hanging indent; nothing on first line foo = long_function_name( var_one, var_two, var_three, var_four) meal = ( spam, beans) # 4-space hanging indent in a dictionary foo = &#123; long_dictionary_key: long_dictionary_value, ... &#125; 1234567891011121314151617No: # Stuff on first line forbidden foo = long_function_name(var_one, var_two, var_three, var_four) meal = (spam, beans) # 2-space hanging indent forbidden foo = long_function_name( var_one, var_two, var_three, var_four) # No hanging indent in a dictionary foo = &#123; long_dictionary_key: long_dictionary_value, ... &#125; 3.6 空行(Blank Lines) 顶层的定义之间空两行， 方法定义之间空一行。 顶层定义之间空两行, 比如函数或者类定义。方法定义，类定义与第一个方法之间，都应该空一行。函数或方法中, 某些地方要是你觉得合适, 就空一行。 3.7 空格(Whitespace) 按照标准的排版规范使用标点两边的空格。 括号内不要有空格。 12Yes: spam(ham[1], &#123;eggs: 2&#125;, [])No: spam( ham[ 1 ], &#123; eggs: 2 &#125;, [ ] ) 不要在逗号，分号，冒号前面加空格，在他们后面加。行尾不用加空格。 123Yes: if x == 4: print(x, y) x, y = y, x 123No: if x == 4 : print(x , y) x , y = y , x 参数列表，索引或切片的左括号前不应该加空格。 1234Yes: spam(1)No: spam (1)Yes: dict['key'] = list[index]No: dict ['key'] = list [index] 在切片中，应该添加适当的空格。 123456Yes:ham[1:9], ham[1:9:3], ham[:9:3], ham[1::3], ham[1:9:]ham[lower:upper], ham[lower:upper:], ham[lower::step]ham[lower+offset : upper+offset]ham[: upper_fn(x) : step_fn(x)], ham[:: step_fn(x)]ham[lower + offset : upper + offset] 12345No:ham[lower + offset:upper + offset]ham[1: 9], ham[1 :9], ham[1:9 :3]ham[lower : : upper]ham[ : upper] 在二元操作符两边都加上一个空格, 比如赋值(=)， 比较(==, &lt;, &gt;, !=, &lt;&gt;, &lt;=, &gt;=, in, not in, is, is not), 布尔(and, or, not)。 12Yes: x == 1No: x&lt;1 当=用于指示关键字参数或默认参数值时，不要在其两侧使用空格。但是在进行类型注释的时候，允许使用空格。 12345Yes: def complex(real, imag=0.0): return Magic(r=real, i=imag)Yes: def complex(real, imag: float = 0.0): return Magic(r=real, i=imag)No: def complex(real, imag = 0.0): return Magic(r = real, i = imag)No: def complex(real, imag: float=0.0): return Magic(r = real, i = imag) 不要用空格来垂直对齐多行间的标记, 因为这会成为维护的负担(适用于:，#，=等)，保持左对齐就可以： 12345678Yes: foo = 1000 # comment long_name = 2 # comment that should not be aligned dictionary = &#123; 'foo': 1, 'long_name': 2, &#125; 12345678No: foo = 1000 # comment long_name = 2 # comment that should not be aligned dictionary = &#123; 'foo' : 1, 'long_name': 2, &#125; 3.8 SheBang行(Shebang Line) 在计算领域中，Shebang(也称为 Hashbang )是一个由井号和叹号构成的字符序列 #! ，其出现在文本文件的第一行的前两个字符。 在文件中存在 Shebang 的情况下，类 Unix 操作系统的程序载入器会分析 Shebang 后的内容，将这些内容作为解释器指令，并调用该指令，并将载有 Shebang 的文件路径作为该解释器的参数。 大部分.py文件不必以#!作为文件的开头。 根据 PEP-394 ，程序的main文件应该以 #!/usr/bin/python2或者 #!/usr/bin/python3开始。 3.9 注释和文档字符串(Comments and Docstrings) 确保正确使用模块、函数、方法的文档字符串和行内注释。 3.9.1 文档字符串(Docstrings) Python有一种独一无二的的注释方式：使用文档字符串。文档字符串是包，模块，类或函数里的第一个语句。这些字符串可以通过对象的__doc__成员被自动提取，并且被pydoc所用。 (你可以在你的模块上运行pydoc试一把， 看看它长什么样)。 我们对文档字符串的惯例是使用三重双引号&quot;&quot;&quot;( PEP-257 )。 一个文档字符串应该这样组织：首先是一行以句号，问号或感叹号结尾的概述(或者该文档字符串单纯只有一行)。接着是一个空行。接着是文档字符串剩下的部分，它应该与文档字符串的第一行的第一个引号对齐。下面有更多文档字符串的格式化规范。 3.9.2 模块(Modules) 每个文件应该包含一个许可证。根据项目使用的许可(例如：Apache 2.0，BSD，LGPL，GPL)，选择合适的许可证。 3.9.3 函数和方法(Functions and Methods) 下文所指的函数，包括函数、方法以及生成器。 一个函数必须要有文档字符串，除非它满足以下条件： 外部不可见 非常短小 简单明了 文档字符串应该包含函数做什么，以及输入和输出的详细描述。通常，不应该描述&quot;怎么做&quot;，除非是一些复杂的算法。文档字符串应该提供足够的信息，当别人编写代码调用该函数时，他不需要看一行代码，只要看文档字符串就可以了。对于复杂的代码，在代码旁边加注释会比使用文档字符串更有意义。 关于函数的几个方面应该在特定的小节中进行描述记录，这几个方面如下文所述。每节应该以一个标题行开始。标题行以冒号结尾。除标题行外，节的其他内容应被缩进2个空格。 Args: 列出每个参数的名字并在名字后使用一个冒号和一个空格，分隔对该参数的描述。如果描述太长超过了单行120字符，使用2或者4个空格的悬挂缩进(与文件其他部分保持一致)。描述应该包括所需的类型和含义。如果一个函数接受foo(可变长度参数列表)或者**bar (任意关键字参数), 应该详细列出foo和**bar。 Returns: (或者 Yields: 用于生成器) 描述返回值的类型和语义。如果函数返回None，这一部分可以省略。 Raises: 列出与接口有关的所有异常。 12345678910111213141516171819202122232425262728293031&gt; def fetch_bigtable_rows(big_table, keys, other_silly_variable=None):&gt; &quot;&quot;&quot;Fetches rows from a Bigtable.&gt; &gt; Retrieves rows pertaining to the given keys from the Table instance&gt; represented by big_table. Silly things may happen if&gt; other_silly_variable is not None.&gt; &gt; Args:&gt; big_table: An open Bigtable Table instance.&gt; keys: A sequence of strings representing the key of each table row&gt; to fetch.&gt; other_silly_variable: Another optional variable, that has a much&gt; longer name than the other args, and which does nothing.&gt; &gt; Returns:&gt; A dict mapping keys to the corresponding table row data&gt; fetched. Each row is represented as a tuple of strings. For&gt; example:&gt; &gt; &#123;&apos;Serak&apos;: (&apos;Rigel VII&apos;, &apos;Preparer&apos;),&gt; &apos;Zim&apos;: (&apos;Irk&apos;, &apos;Invader&apos;),&gt; &apos;Lrrr&apos;: (&apos;Omicron Persei 8&apos;, &apos;Emperor&apos;)&#125;&gt; &gt; If a key from the keys argument is missing from the dictionary,&gt; then that row was not found in the table.&gt; &gt; Raises:&gt; IOError: An error occurred accessing the bigtable.Table object.&gt; &quot;&quot;&quot;&gt; pass&gt; 3.9.4 类(Classes) 类应该在其定义下有一个用于描述该类的文档字符串。如果你的类有公共属性(Attributes)，那么文档中应该有一个属性(Attributes)段。并且应该遵守和函数参数相同的格式。 12345678910111213141516171819&gt; class SampleClass(object):&gt; &quot;&quot;&quot;Summary of class here.&gt; &gt; Longer class information....&gt; Longer class information....&gt; &gt; Attributes:&gt; likes_spam: A boolean indicating if we like SPAM or not.&gt; eggs: An integer count of the eggs we have laid.&gt; &quot;&quot;&quot;&gt; &gt; def __init__(self, likes_spam=False):&gt; &quot;&quot;&quot;Inits SampleClass with blah.&quot;&quot;&quot;&gt; self.likes_spam = likes_spam&gt; self.eggs = 0&gt; &gt; def public_method(self):&gt; &quot;&quot;&quot;Performs operation blah.&quot;&quot;&quot;&gt; 3.9.5 块注释和行注释(Block and Inline Comments) 最需要写注释的是代码中那些技巧性的部分。如果你在下次 代码审查 的时候必须解释一下，那么你应该现在就给它写注释。对于复杂的操作，应该在其操作开始前写上若干行注释。对于不是一目了然的代码，应在其行尾添加注释。 1234567&gt; # We use a weighted dictionary search to find out where i is in&gt; # the array. We extrapolate position based on the largest num&gt; # in the array and the array size and then do binary search to&gt; # get the exact number.&gt; &gt; if i &amp; (i-1) == 0: # true iff i is a power of 2&gt; 为了提高可读性，注释应该至少离开代码2个空格。 另一方面，绝不要描述代码。 假设阅读代码的人比你更懂Python， 他只是不知道你的代码要做什么。 123&gt; # BAD COMMENT: Now go through the b array and make sure whenever i occurs&gt; # the next element is i+1&gt; 3.9.6 标点符合、拼写和语法(Punctuation, Spelling and Grammar) 大家都喜欢读一份写得很优雅的注释，而不是杂乱没有头绪的注释。注释应该是一段通顺的语句。完整的句子比一个片段可读性更强。 源代码保持高度的清晰度和可读性是非常重要的。适当的标点符号、词汇选择和语法是必要的。 3.10 类(Classes) Python3起，默认的类都继承自object，不用再显式继承object。 12345678Okay: class SampleClass: pass class OuterClass: class InnerClass: pass 3.11 字符串(Strings) 使用%操作符或者格式化方法format格式化字符串。Python3.6提供了f-string ，参考PEP-498。简单的情况使用+。 123456Yes: x = a + b x = '%s, %s!' % (imperative, expletive) x = '&#123;&#125;, &#123;&#125;'.format(first, second) x = 'name: %s; score: %d' % (name, n) x = 'name: &#123;&#125;; score: &#123;&#125;'.format(name, n) x = f'name: &#123;name&#125;; score: &#123;n&#125;' # Python 3.6+ 1234No: x = '%s%s' % (a, b) # use + in this case x = '&#123;&#125;&#123;&#125;'.format(a, b) # use + in this case x = first + ', ' + second x = 'name: ' + name + '; score: ' + str(n) 避免在循环中用+和+=操作符来累加字符串。 由于字符串是不可变的，这样做会创建不必要的临时对象，并且导致二次方而不是线性的运行时间。作为替代方案，你可以将每个子串加入列表，然后在循环结束后用 .join 连接列表。 (也可以将每个子串写入一个 cStringIO.StringIO 缓存中。) 虽然Cpython对+和+=进行了优化，速度比join还快，但是不建议依赖特定的解释器进行编程。 12345Yes: items = ['&lt;table&gt;'] for last_name, first_name in employee_list: items.append('&lt;tr&gt;&lt;td&gt;%s, %s&lt;/td&gt;&lt;/tr&gt;' % (last_name, first_name)) items.append('&lt;/table&gt;') employee_table = ''.join(items) 1234No: employee_table = '&lt;table&gt;' for last_name, first_name in employee_list: employee_table += '&lt;tr&gt;&lt;td&gt;%s, %s&lt;/td&gt;&lt;/tr&gt;' % (last_name, first_name) employee_table += '&lt;/table&gt;' 在同一个文件中， 保持使用字符串引号的一致性。要么都是双引号，要么都是单引号， 在遇见字符串中有引号时，使用另一种引号。 1234Yes: Python('Why are you hiding your eyes?') Gollum("I'm scared of lint errors.") Narrator('"Good!" thought a happy Python reviewer.') 1234No: Python("Why are you hiding your eyes?") Gollum('The lint. It burns. It burns us.') Gollum("Always the great lint. Watching. Watching.") 为多行字符串使用三重双引号&quot;&quot;&quot;而非三重单引号’’’。多行字符串与程序其他部分的缩进方式不一致， 使用隐式行链接来替代。 123Yes:print("This is much nicer.\n" "Do it this way.\n") 1234 No: print("""This is pretty ugly.Don't do this.""") 3.12 文件和sockets(Files and Sockets) 在文件和sockets结束时，显式的关闭它。 除文件外，sockets或其他类似文件的对象在没有必要的情况下打开，会有许多副作用，例如： 它们可能会消耗有限的系统资源，如文件描述符。如果这些资源在使用后没有及时归还系统，那么用于处理这些对象的代码会将资源消耗殆尽。 持有文件将会阻止对于文件的其他诸如移动、删除之类的操作。 仅仅是从逻辑上关闭文件和sockets，那么它们仍然可能会被其共享的程序在无意中进行读或者写操作。 只有当它们真正被关闭后，对于它们尝试进行读或者写操作将会抛出异常，并使得问题快速显现出来。 而且，对文件对象析构时，文件和sockets会自动关闭。 不同的Python解释器实现了不同的gc。 对于文件意外的引用,会导致对于文件的持有时间超出预期(比如对于异常的跟踪, 包含有全局变量等). 推荐使用with管理文件： 123with open("hello.txt") as hello_file: for line in hello_file: print line 对于不支持with的类文件对象，使用contextlib.closing()： 12345import contextlibwith contextlib.closing(urllib.urlopen("http://www.python.org/")) as front_page: for line in front_page: print(line) 3.13 TODO注释(TODO Comments) 为临时代码使用TODO注释，它是一种短期解决方案。 TODO注释应该在所有开头处包含&quot;TODO&quot;字符串，紧跟着是用括号括起来的你的名字、email地址或其它标识符。然后是一个可选的冒号，接着必须有一行注释， 解释要做什么。 主要目的是为了有一个统一的TODO格式， 这样添加注释的人就可以搜索到他的注释。 写了TODO注释并不保证写的人会亲自解决问题。当你写了一个TODO，请注上你的名字。 12# TODO(kl@gmail.com): Use a "*" here for string repetition.# TODO(Zeke) Change this to use relations. 3.14 导入格式化(Imports formatting) 导入需要独占一行。 12Yes: import os import sys 1No: import os, sys 导入总应该放在文件顶部，位于模块注释和文档字符串之后，模块全局变量和常量之前。导入应该按照以下顺序分组: 标准库导入，比如：import sys 第三方库导入，比如：import tensorflow as tf 应用程序指定导入，比如：from otherproject.ai import mind 导入的每一部分需要按字典序排序，忽略大小写。 123456789101112131415import collectionsimport Queueimport sysimport argcompleteimport BeautifulSoupimport cryptographyimport tensorflow as tffrom otherproject.ai import bodyfrom otherproject.ai import mindfrom otherproject.ai import soulfrom myproject.backend.hgwells import time_machinefrom myproject.backend.state_machine import main_loop 3.15 语句(Statements) 通常每个语句独占一行。 不过， 如果仅仅只有if语句，可以把整个语句放在一行。 如果为了测试，临时这么做也是可以的。 1234Yes: if foo: bar(foo) import pdb; pdb.set_trace() 1234567891011No: if foo: bar(foo) else: baz(foo) try: bar(foo) except ValueError: baz(foo) try: bar(foo) except ValueError: baz(foo) 3.16 访问控制(Access Control) 使用@property对属性进行控制，不要另外写getter和setter方法。 例外：如果逻辑非常复杂，或者调用非常频繁影响到性能，那么可以使用函数调用，需要遵守命名规则比如set_foo() get_foo()。 3.17 命名(Naming) Python命名分为许多种类： module_name, package_name, ClassName, method_name, ExceptionName, function_name, GLOBAL_CONSTANT_NAME,global_var_name, instance_var_name, function_parameter_name, local_var_name。 3.17.1应该避免的名称 单字符名称， 除了计数器和迭代器。 包/模块名中的连字符(-)。 双下划线开头并结尾的名称(除了Python保留的例如__init__)。 3.17.2 命名约定 所谓&quot;内部(Internal)&quot;表示仅模块内可用，或者，在类内是保护或私有的。 用单下划线(_)开头表示模块变量或函数是protected的(使用import * from时不会包含)。 用双下划线(__)开头的实例变量或方法表示类内私有。 将相关的类和顶层函数放在同一个模块里。不像Java，没必要限制一个类一个模块。 对类名使用大写字母开头的单词(如CapWords, 即Pascal风格)，但是模块名应该用小写加下划线的方式(如lower_with_under.py)。尽管已经有很多现存的模块使用类似于CapWords.py这样的命名，但现在已经不鼓励这样做，因为如果模块名碰巧和类名一致，这会让人困扰。 3.17.3 文件命名 Python文件名必须是.py的扩展名，不能包含连字符(-)。因此这些文件可以被导入和单元测试。如果你想要一个没有扩展名的可执行文件，使用软链接或者是一个bash来包装 exec &quot;$0.py&quot; &quot;$@&quot;。 3.17.4 Guido van Rossum推荐的命名规范 类型(Type) 公共(Public) 内部(Internal) Packages lower_with_under Modules lower_with_under _lower_with_under Classes CapWords _CapWords Exceptions CapWords Functions lower_with_under() _lower_with_under() Global/Class Constants CAPS_WITH_UNDER _CAPS_WITH_UNDER Global/Class Variables lower_with_under _lower_with_under Instance Variables lower_with_under _lower_with_under (protected) Method Names lower_with_under() _lower_with_under() (protected) Function/Method Parameters lower_with_under Local Variables lower_with_under Python 通过双下划线命名来支持私有变量。但是最好使用单下划线来作为内部变量访问。 3.18 主函数(Main) 每一个可执行脚本都应该能被导入，并且导入不会执行这个脚本的主要功能部分。 主要功能部分应该被放在主函数中。 代码在执行主程序前应该检查if __name__ == '__main__'，这样当模块被导入时，这部分代码就不会被执行。 12345def main(): ...if __name__ == '__main__': main() 所有的顶层代码在模块导入时都会被执行。因此要小心在全局中调用函数，创建对象的操作，需要测试的代码写在if __name__ == '__main__'下面。 3.19 函数长度(Function length) 推荐使用小而精的函数。 有时候一段比较长的函数是比较合适的，所以这里不会过多地限制函数的长度。如果一个函数超过40行，那么要考虑一下它是否能被合理地拆分。一个很长的函数可能在这个需求下是适用的，并且没有bug，但是如果新增了需求需要对它进行改动，在多次改动后可能会导致难以找到的bug。函数应该是简洁明了，易于他人维护的。 如果你正好遇到一个函数代码量非常地长并且复杂，那么最好先把它拆分为多个函数易于管理。不要觉得拆分是一件很困难的事情，为了你的长远考虑，我建议你这么做。 3.20 类型注释(Type Annotations) 3.20.1 一般规则 了解PEP-484。 不要在方法中注释self和cls。 如果变量或返回值不明确，使用Any。 不要求注释所有的函数。 至少注释公共API。 在明确和灵活之间做一个平衡。 注释的代码相对难理解。 注释代码是一个稳妥的做法，对一段成熟的代码一定要进行注释。 注释代码可以发现类型相关的错误。 5 临别赠言 务必保持代码的一致性 如果你正在编辑代码，花几分钟看一下周边代码，然后决定风格。如果它们在所有的算术操作符两边都使用空格，那么你也应该这样做，如果它们的注释都用标记包围起来，那么你的注释也要这样。 制定风格指南的目的在于让代码有规可循，这样人们就可以专注于&quot;你在说什么&quot;，而不是&quot;你在怎么说&quot;。我们在这里给出的是全局的规范，但是本地的规范同样重要。如果你加到一个文件里的代码和原有代码大相径庭，它会让读者不知所措，请避免这种情况。 作者：倪侃（859905874@qq.com）]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>codestyle python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简历]]></title>
    <url>%2F2018%2F07%2F29%2FMe%2F%E5%85%B3%E4%BA%8E%E6%88%91%2F</url>
    <content type="text"><![CDATA[基本信息 ​ Github: https://github.com/nikan1996 ​ Email: 859905874@qq.com 教育经历 ​ 2014年9月-2018年7月本科毕业于浙江工业大学。 实习经历 2017年3月-2017年9月 数梦工场，职位：后台开发工程师。 2017年10月-2018年6月 艾耕科技，职位：爬虫开发工程师。 工作经历 2018年7月-2019年1月 艾耕科技， 职位：数据平台开发工程师。 兴趣爱好 健身 音乐 未完待续…]]></content>
      <tags>
        <tag>resume</tag>
      </tags>
  </entry>
</search>
